% vim: wrap cursorline
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL report/emse24.tex           Sat Oct 12 16:17:17 2024
%DIF ADD report/emse24-revised.tex   Sun Mar  9 14:10:42 2025

\RequirePackage{fix-cm}
%
% \documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
%% \usepackage{hyperref}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[most]{tcolorbox}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{framed}
\newcommand{\highlight}[1]{\begin{framed}%
  \noindent#1
\end{framed}}

\definecolor{backcolor}{rgb}{0.95,0.95,0.92}
\colorlet{punct}{red!60!black}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinestyle{mystyle}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{delim},
  stringstyle=\color{punct},
  commentstyle=\color{numb},
  breakatwhitespace=true,
  breaklines=true,
  keepspaces=true,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  stepnumber=1,
  numbersep=3pt,
  captionpos=b,
  % numbers=left,
  % numberstyle=\tiny\color{gray},
  backgroundcolor=\color{backcolor},
  % aboveskip=0pt,
  % belowskip=0pt
}
\lstset{style=mystyle}

\lstset{emph={%
    assert%
    },emphstyle={\color{delim}}%
}%

%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFadd}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{{\protect\color{red}\sout{#1}}} %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF COLORLISTINGS PREAMBLE %DIF PREAMBLE
\RequirePackage{listings} %DIF PREAMBLE
\RequirePackage{color} %DIF PREAMBLE
\lstdefinelanguage{DIFcode}{ %DIF PREAMBLE
%DIF DIFCODE_UNDERLINE %DIF PREAMBLE
  moredelim=[il][\color{red}\sout]{\%DIF\ <\ }, %DIF PREAMBLE
  moredelim=[il][\color{blue}\uwave]{\%DIF\ >\ } %DIF PREAMBLE
} %DIF PREAMBLE
\lstdefinestyle{DIFverbatimstyle}{ %DIF PREAMBLE
	language=DIFcode, %DIF PREAMBLE
	basicstyle=\ttfamily, %DIF PREAMBLE
	columns=fullflexible, %DIF PREAMBLE
	keepspaces=true %DIF PREAMBLE
} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim}{\lstset{style=DIFverbatimstyle}}{} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim*}{\lstset{style=DIFverbatimstyle,showspaces=true}}{} %DIF PREAMBLE
\lstset{extendedchars=\true,inputencoding=utf8}

%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

\title{Understanding Feedback Mechanisms in Machine Learning Jupyter Notebooks}

%\titlerunning{Short form of title}        % if too long for running head

\author{Arumoy Shome\and
	Lu{\'\i}s Cruz\and
	Diomidis Spinellis\and
	Arie van Deursen
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{A. Shome \at
	Delft University of Technology\\
	\email{a.shome@tudelft.nl}
	\and
	L. Cruz \at
	Delft University of Technology\\
	\email{l.cruz@tudelft.nl}
	\and
	D. Spinellis \at
	Delft University of Technology\\
	\email{d.spinellis@tudelft.nl}
	\and
	A. V. Deursen \at
	Delft University of Technology\\
	\email{arie.vandeursen@tudelft.nl}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
	The machine learning development lifecycle is characterized by iterative and exploratory processes that rely on feedback mechanisms to ensure data and model integrity. Despite the critical role of feedback in machine learning engineering, no prior research has been conducted to identify and understand these mechanisms. To address this knowledge gap, we mine 297.8 thousand Jupyter notebooks and \DIFdelbegin \DIFdel{analyse }\DIFdelend \DIFaddbegin \DIFadd{analyze }\DIFaddend 2.3 million code cells. We identify three key feedback mechanisms---assertions, print statements and last cell statements---and further categorize them into implicit and explicit forms of feedback. Our findings reveal extensive use of implicit feedback for critical design decisions and the relatively limited adoption of explicit feedback mechanisms. By conducting detailed case studies with selected feedback instances, we uncover the potential for automated validation of critical assumptions in ML workflows using assertions. Finally, this study underscores the need for improved documentation, and provides practical recommendations on how existing feedback mechanisms in the ML development workflow can be effectively used to mitigate technical debt and enhance reproducibility.
\end{abstract}

\keywords{SE4AI, Feedback Mechanisms, Jupyter notebooks, Assertions, Technical Debt}

\section{Introduction}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend The machine learning development lifecycle (MLDL) encompasses a multifaceted series of entangled stages. Significant effort is invested in gathering and assembling the required data from diverse sources. The data then undergoes cleaning and preprocessing to ensure high quality and usability, in the subsequent stages. MLDL is dominated by the model development phase which is highly experimental and iterative. This stage involves exploratory data analysis (EDA) to uncover underlying patterns, fitting the data to various models, and rigorously \DIFdelbegin \DIFdel{analysing }\DIFdelend \DIFaddbegin \DIFadd{analyzing }\DIFaddend the results. Often, this stage may lead to additional rounds of feature engineering, where new attributes are created based on insights gained from initial models to enhance model performance. Practitioners may also engage in comparing and contrasting different models to identify the most effective approach, followed by fine-tuning selected models to optimize performance. The final model is then deployed into production where it is continuously monitored, and periodically retrained to adapt to new data or changing conditions~\citep{haakman2021ai,amershi2019software,sculley2015hidden}.

The MLDL is fundamentally iterative, closely resembling Agile software engineering principles, which emphasize incorporating early feedback and working in small, manageable iterations~\citep{betz2018managing}. Similarly, CRISP-DM---the dominant process model in data mining---outlines a cyclical process that evolves from understanding the business objectives and data, to data preparation, modelling, evaluation and finally deployment and monitoring~\citep{martinez-plumed2021crisp-dm}. Both systems advocate for a dynamic, iterative approach that enhances adaptability and effectiveness in dealing with complex, changing systems like those encountered in ML projects.

Computational notebooks---specifically Jupyter notebooks\footnote{https://jupyter.org/}---have been ubiquitously adopted by the ML community for developing ML enabled systems~\citep{pimentel2019large-scale,quaranta2021kgtorrent,psallidas2019data,perkel2018why}. Jupyter notebooks provide an interactive computing environment that allow users to break the development of complex workflows into smaller, more manageable code chunks. Each notebook is composed of a series of code cells, which can be executed independently in a read-eval-print loop (REPL) style of interactive software development. This REPL approach allows users to write a piece of code, run it, and immediately see the results thus facilitating an incremental and iterative style of software development. This modular structure of notebooks provides users the flexibility to experiment with different approaches and algorithms. Users can test hypotheses, debug issues, and make incremental changes with immediate feedback. The ability to visualize data and results inline and document the process in a narrative form further enhances their utility~\citep{kery2018story,head2019managing,rule2018exploration,chattopadhyay2020whats}.

\DIFaddbegin \DIFadd{Within this environment, feedback mechanisms are essential for improving code quality and diagnosing issues. }\textbf{\DIFadd{Feedback mechanisms are tools or techniques intentionally integrated into the code by the developer to gain insight into the execution of the program}}\DIFadd{. Importantly, not all forms of feedback quality as feedback mechanisms. For example, a bug in the code may produce feedback indicating an issue, but because the bug was not intentionally inserted, it does not count as a feedback mechanism.
}

\DIFaddend Notebook users can obtain feedback using implicit or explicit mechanisms. \DIFdelbegin \DIFdel{Implicit feedback mechanisms require manual validation by the user}\DIFdelend \DIFaddbegin \textbf{\DIFadd{Implicit feedback mechanisms require manual validation by the user}}\DIFaddend . For instance, a common prerequisite for all ML models is that the features in the dataset are all numerical. This can be verified with a print statement in a code cell and manually checking the output. This code statement however, will continue to execute even if the data type of a feature changes in subsequent batches of training data.

In contrast, \DIFdelbegin \DIFdel{explicit feedback mechanisms document the expected conditions of the code and halt execution if these conditions are not met}\DIFdelend \DIFaddbegin \textbf{\DIFadd{explicit feedback mechanisms document the expected conditions of the code and halt execution if these conditions are not met}}\DIFaddend . For instance, the implicit validation method mentioned above can be replaced with an explicit assertion: \lstinline{assert all(df[col].dtype in ['int64', 'float64'] for col in df.columns), "Not all columns are of numerical type"}. This assert statement will stop the execution if the condition is not satisfied and provide immediate feedback with a clear message explaining the issue.

\DIFaddbegin \DIFadd{It is important to contextualize feedback mechanisms within the broader framework of software testing. Traditional testing methodologies employ formal tests to explicitly identify discrepancies between expected and actual code functionality. }\textbf{\DIFadd{While all tests serve as feedback mechanisms, not all feedback mechanisms operate within this formal testing paradigm}}\DIFadd{. For instance, print statements and outputs from the last statement of code cells offer a more informal approach to gaining insights into the execution status and data flow within a Jupyter notebook. While such implicit feedback types can help practitioners diagnose issues within their code, they necessitate manual validation and interpretation, which can increase the potential for oversight and errors.
}

\DIFaddend As machine learning pipelines developed within Jupyter notebooks become business-critical, they are frequently transitioned into automated scripts for production environments~\citep{kery2018story,rule2018exploration}. In these contexts, it is crucial to shift from implicit feedback mechanisms to explicit ones. This transition reduces the reliance on manual verification, thereby enhancing the robustness and maintainability of ML pipelines.

To the best of our knowledge, no prior studies have been conducted to understand how feedback mechanisms are used by practitioners when developing ML pipelines inside Jupyter notebooks. To address this gap, we conduct a series of empirical studies to understand the role of various feedback mechanisms within Jupyter notebooks. We focus on implicit form of feedback obtained from print statements and output generated by the last statement of code cells, as well as explicit form of feedback obtained from assertions.
\DIFaddbegin 

\DIFaddend The research questions along with the contributions of this study are presented below.

\begin{description}
	\item[RQ1.] \textbf{What feedback mechanisms are employed in the development of ML Jupyter notebooks?}

	      We mine 297.8 thousand Jupyter notebooks written in Python from GitHub and Kaggle. We \DIFdelbegin \DIFdel{analyse }\DIFdelend \DIFaddbegin \DIFadd{analyze }\DIFaddend 2.3 million code cells obtained from the notebooks and identify three key feedback mechanisms used in ML notebooks---explicit feedback from assertions and implicit feedback from print and last cell statements.

	      We create a public dataset of 89.6 thousand assertions, 1.4 million print statements and 1 million last cell statements. We perform a descriptive and lexical analysis of the dataset and report the key characteristics of feedback mechanisms used in ML Jupyter notebooks. The dataset along with the replication package is shared publicly under the Creative Commons Attribution (CC-BY) licence\footnote{https://doi.org/10.6084/m9.figshare.26372140.v1}.

	\item[RQ2.] \textbf{How is explicit feedback from assert statements used to validate ML code written in Jupyter notebooks?}

	      To gain a deeper understanding of Python assertions written in \DIFdelbegin \DIFdel{Jupter }\DIFdelend \DIFaddbegin \DIFadd{Jupyter }\DIFaddend notebooks, we conduct individual case-studies with 82 assert statements. We \DIFdelbegin \DIFdel{analyse }\DIFdelend \DIFaddbegin \DIFadd{analyze }\DIFaddend the assertion along with the surrounding code to understand its purpose. Additionally, we \DIFdelbegin \DIFdel{analyse }\DIFdelend \DIFaddbegin \DIFadd{analyze }\DIFaddend the entire code cell, the previous and next cells along with the purpose of the notebook to bring in rich context.

	\item[RQ3.] \textbf{How is implicit feedback from print statements and last cell statements used when writing ML code in Jupyter notebooks?}

	      Similarly, we conduct individual case-studies with 44 print statements and 27 last cell statements. During each case-study, we \DIFdelbegin \DIFdel{analyse }\DIFdelend \DIFaddbegin \DIFadd{analyze }\DIFaddend the code statement along with the raw output to understand its purpose.
\end{description}

Our findings indicate that implicit feedback mechanisms are extensively utilized for making critical design and implementation decisions during the MLDL. While manual validation of these outputs is common, it is also prone to inconsistencies thus highlighting the need for automated validation mechanisms. Assertions are identified as a potential solution, capable of verifying data distributions and ensuring model performance metrics meet predefined thresholds. However, the study also notes the limited use of assertions, with only 8.5\% of the notebooks \DIFdelbegin \DIFdel{analysed }\DIFdelend \DIFaddbegin \DIFadd{analyzed }\DIFaddend incorporating them, suggesting a significant gap in current testing practices within the ML community. The findings underscore the necessity for developing ML-specific testing tools that are deeply integrated into the ML development workflow to ensure consistent and reliable validation processes.

\section{Background}\label{sec:background}

This section introduces relevant prior concepts for this study.

\subsection{REPLs, Interactive Computing and Project Jupyter}

A Read-Eval-Print Loop (REPL) is an interactive programming environment that takes single user inputs (Read), executes them (Eval), returns the result to the user (Print) and then waits for the next command (Loop). REPLs offer an immediate feedback loop for quickly testing and debugging code snippets. The Python standard distribution includes a Python REPL with a command line interface to execute Python code in real-time. IPython---a more advanced alternative to the Python REPL---extends the capabilities of the standard Python shell with additional features such as powerful introspection, additional shell syntax, tab completion, and command history. Jupyter notebooks (and its successor Jupyter lab) leverage the IPython kernel to execute Python code and \DIFdelbegin \DIFdel{extends }\DIFdelend \DIFaddbegin \DIFadd{extend }\DIFaddend its capabilities with a graphical user interface (GUI). Resembling the literate programming paradigm~\citep{knuth1984literate}, Jupyter notebooks offer a web-based interface where users can create and share documents that contain live code, equations, visualizations, and narrative text.

\subsection{Anatomy of Jupyter Notebooks}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{nb.pdf}
	\caption{Example Jupyter notebook with code and markdown cells, adapted from~\citep[Figure~1]{pimentel2019large-scale}.}
	\label{fig:nb}
\end{figure}

Figure~\ref{fig:nb} shows an example Jupyter notebook with markdown and code cells. The data comprised within Jupyter notebooks is stored using the JSON schema specified by \emph{nbformat}\footnote{https://nbformat.readthedocs.io/en/latest/index.html}, and thus can be parsed programmatically. A notebook consists of \emph{cells} which can be of three types---``markdown'', ``code'' and ``raw''. All cells of a notebook are stored as a list under the top-level \lstinline[language={}]$cells$ key in the JSON representation. Within this list, each cell is represented as a JSON object. The contents of the cell are stored as a list of strings inside the \lstinline[language={}]$source$ key.

Only code cells in Jupyter notebooks can generate outputs. As shown in Figure~\ref{fig:nb}, outputs can come from three main sources in the source code---print statements, the last statement in the code cell, and statements that create a visualization. The outputs produced by code cells are stored as JSON objects under the \lstinline[language={}]$outputs$ key in the corresponding JSON representation of the notebook. Text outputs (such as those produced by the last statement of a cell or print statements) are stored under the \lstinline[language={}]$text$ key. Visual outputs have a sightly different JSON schema and are stored under the \lstinline[language={}]$data$ key. The image itself is represented as a base64 encoded string which is stored inside the \lstinline[language={}]$data$ object, under the \lstinline[language={}]$image/png$ key.

\subsection{Python Abstract Syntax Tree}

The Python \lstinline{ast} module enables the parsing, manipulation, and analysis of Python source code by converting it into an abstract syntax tree (AST)\footnote{https://docs.python.org/3/library/ast.html}. The \lstinline{ast} module can parse source code strings into AST objects, traverse and modify the tree and compile it back into executable code, providing a powerful tool for dynamic code manipulation. In Python, \lstinline{assert} statements are represented in the AST by the \lstinline{Assert} class. The \lstinline{Assert} node includes the \lstinline{test} attribute---which contains the condition being asserted---and the \lstinline{msg} attribute---which holds an optional message displayed if the assertion fails. Print statements are treated as a call to the built-in \lstinline{print} function. Function calls are represented in the AST by the \lstinline{Call} class. The \lstinline{Call} node includes the \lstinline{func} attribute representing the \lstinline{print} function, and \lstinline{args} which lists the arguments passed.

\section{Methodology}

\subsection{Data Collection}\label{sec:data-collect}

%DIF >  TODO: update figure, 190861 notebooks included in analysis
%DIF >  NOTE: above number extracted using the following shell command: `cat data-collection.log |grep '^OUTPUT'|cut -d: -f2|sed -e 's/-asserts\.csv//' -e 's/-prints\.csv//' -e 's/-lasts\.csv//'|sort|uniq|wc -l`
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{data-collection.pdf}
	\caption{Overview of data collection methodology used in this study}
	\label{fig:data-collection}
\end{figure}

Figure~\ref{fig:data-collection} illustrates the data collection process employed in this study to gather Jupyter notebooks written in Python from GitHub and Kaggle. \DIFaddbegin \DIFadd{GitHub is a popular platform to host and collaborate on software projects. It offers extensive tools for project management such as issue tracking, pull requests, code reviews and tooling for continuous integration and delivery. In contrast, Kaggle is focused specifically on data science and machine learning. It provides integrated computational resources, a rich repository of datasets, and a platform to host machine learning competitions. While GitHub supports formal project workflows, Kaggle encourages a more informal approach to data exploration and modeling, making it a preferred choice for data science practitioners looking for hands-on experience with developing machine learning models.
}

\subsubsection{\DIFadd{Collection of ML Jupyter Notebooks}}

\DIFaddend We used GitHub's advanced search syntax\footnote{https://docs.github.com/en/search-github/searching-on-github/searching-code}, with the following search query: \lstinline[language={}]$language:"Jupyter Notebook"$ to mine 49.9 thousand Jupyter notebooks from public repositories on GitHub\footnote{Collected on June 22, 2023}. We used the pre-existing dataset KGTorrent~\citep{quaranta2021kgtorrent} for Kaggle since it does not support advanced code-based search like GitHub. To the best of our knowledge, KGTorrent is the largest dataset of Python Jupyter notebooks obtained from Kaggle, consisting of 247.9 thousand notebooks. Consequently, we started with 283 GB of data comprising 297.8 thousand Python Jupyter notebooks.

\DIFdelbegin \DIFdel{Since the focus of this study is on the analysis of Python code , we only include notebooks that have a valid underlying JSON structure and contain }\DIFdelend \DIFaddbegin \DIFadd{This study focuses on analyzing Python code in Jupyter notebooks with valid JSON structures containing }\DIFaddend at least one code cell. \DIFdelbegin \DIFdel{Finally, to focus on Python code written specifically for ML }\DIFdelend \DIFaddbegin \DIFadd{To concentrate specifically on machine learning }\DIFaddend projects, we only \DIFdelbegin \DIFdel{include }\DIFdelend \DIFaddbegin \DIFadd{included }\DIFaddend notebooks that import at least one \DIFdelbegin \DIFdel{of the following popular ML libraries---Scikit }\DIFdelend \DIFaddbegin \DIFadd{major ML library: Scikit }\DIFaddend Learn~\citep{pedregosa2011scikit-learn}, \DIFdelbegin \DIFdel{Pytorch}\DIFdelend \DIFaddbegin \DIFadd{PyTorch}\DIFaddend ~\citep{paszke2017automatic}, \DIFdelbegin \DIFdel{Tensorflow~\mbox{%DIFAUXCMD
\citep{abadi2015tensorflow}}\hskip0pt%DIFAUXCMD
and }\DIFdelend \DIFaddbegin \DIFadd{TensorFlow~\mbox{%DIFAUXCMD
\citep{abadi2015tensorflow}}\hskip0pt%DIFAUXCMD
, or }\DIFaddend Keras~\citep{chollet2015keras}. \DIFdelbegin \DIFdel{The librariesare selected based on the import analysis conducted by \mbox{%DIFAUXCMD
\citet{psallidas2019data} }\hskip0pt%DIFAUXCMD
on 6 million Jupyter notebooks collected from GitHub. We use the Python }\DIFdelend \DIFaddbegin \DIFadd{While this approach may exclude some ML projects using alternative libraries, research by \mbox{%DIFAUXCMD
\citet{psallidas2019data} }\hskip0pt%DIFAUXCMD
examining six million GitHub Jupyter notebooks confirms that most ML-oriented notebooks utilize these frameworks. Despite their widespread usage, we deliberately excluded Pandas, NumPy, and SciPy from our selection criteria as these libraries serve numerous purposes beyond machine learning and are not specifically designed for model training. After applying these filters, we excluded 107 thousand notebooks, resulting in a final dataset of 190.8 thousand notebooks for analysis.
}

\subsubsection{\DIFadd{Collection of Feedback Mechanisms}}

%DIF >  TODO: make sure that tense is consistent, here its present perfect but before I used past I think
\DIFadd{For RQ1, we extracted the contents of all code cells from the Jupyter notebooks by parsing the underlying JSON structure. We then used the }\DIFaddend \lstinline{ast} module \DIFaddbegin \DIFadd{provided by the Python programming language }\DIFaddend to extract the assert, print and last statements from all code cells.
\DIFaddbegin 

\DIFadd{The raw contents of each cell were then converted into an AST. The }{\color{blue}%DIFAUXCMD
\lstinline{NodeVisitor} %
}%DIFAUXCMD
\DIFadd{class from the }{\color{blue}%DIFAUXCMD
\lstinline{ast} %
}%DIFAUXCMD
\DIFadd{module was used to traverse the AST and collect all }{\color{blue}%DIFAUXCMD
\lstinline{Assert} %
}%DIFAUXCMD
\DIFadd{nodes. To collect assertions written using Python modules not included in the standard distribution, we also collected all function }{\color{blue}%DIFAUXCMD
\lstinline{Call} %
}%DIFAUXCMD
\DIFadd{nodes that contain the string ``assert'' in their name. Similarly, all print statements were collected by traversing the AST to find all function }{\color{blue}%DIFAUXCMD
\lstinline{Call} %
}%DIFAUXCMD
\DIFadd{nodes with the name ``print''. To collect the last statements, we first removed all cells that did not produce any output and consequently extracted the last top-level node from the AST of the code cell. The raw Python code was regenerated from the ASTs using the }{\color{blue}%DIFAUXCMD
\lstinline{unparse} %
}%DIFAUXCMD
\DIFadd{method provided by the }{\color{blue}%DIFAUXCMD
\lstinline{ast} %
}%DIFAUXCMD
\DIFadd{module.
}

\DIFaddend Prior to conducting the analysis for RQ2 and RQ3, we \DIFdelbegin \DIFdel{remove }\DIFdelend \DIFaddbegin \DIFadd{removed }\DIFaddend duplicate data points resulting in a data set of 27.1 thousand assertions, 498.3 thousand print statements and 373.6 thousand last cell statements.

\subsection{\DIFaddbegin \DIFadd{Exploratory }\DIFaddend Case \DIFdelbegin \DIFdel{Studies}\DIFdelend \DIFaddbegin \DIFadd{Study Analysis}\DIFaddend }

For RQ2 and RQ3, we allocated a fixed time resource of 200 hours to conduct the \DIFaddbegin \DIFadd{exploratory }\DIFaddend case study analysis of all candidate assertions, print statements and last cell statements. \DIFdelbegin \DIFdel{To surface interesting candidates for the case studies, we }\DIFdelend \DIFaddbegin \DIFadd{We specified Jupyter notebooks written in the Python programming language as the }\emph{\DIFadd{site}} \DIFadd{for the study. Given the exploratory nature of this study, we emphasized capturing a diverse range of candidates by }\DIFaddend apply text processing \DIFaddbegin \DIFadd{and sampling }\DIFaddend techniques as described below.

The assertions, print statements and last cell statements are first tokenized---special characters and alphanumeric words shorter than two characters are removed. Two stop words namely ``assert'' and ``print'' are removed since they appear in all assertions and print statements respectively. The term frequency (TF) for \DIFdelbegin \DIFdel{all }\DIFdelend \DIFaddbegin \DIFadd{the remaining }\DIFaddend tokens is calculated and then normalized using their inter-document frequency (IDF)\DIFdelbegin \DIFdel{such that tokens that appear less frequently are assigned a higher value . We apply }\DIFdelend \DIFaddbegin \DIFadd{. This normalization process assigns higher values to tokens that are less frequently encountered, which can lead to overemphasis of candidates that contain rare tokens. Consider for instance, the trivial print statement }{\color{blue}%DIFAUXCMD
\lstinline{print("hello, world!")}%
}%DIFAUXCMD
\DIFadd{. Since this statement does not commonly appear in the context of machine learning Jupyter notebooks, it will have a high TF-IDF score but adds little substantive value to the analysis.
}

\DIFadd{To address the potential bias introduced by this method, we employ }\DIFaddend stratified random sampling to \DIFdelbegin \DIFdel{identify the }\DIFdelend \DIFaddbegin \DIFadd{select }\DIFaddend candidates for the case study analysis. The subgroups are created \DIFdelbegin \DIFdel{by adding }\DIFdelend \DIFaddbegin \DIFadd{based on the aggregated }\DIFaddend TF-IDF \DIFaddbegin \DIFadd{values }\DIFaddend of the tokens in each candidate\DIFdelbegin \DIFdel{to produce an aggregate value}\DIFdelend . The candidates are then divided into quartiles based on the aggregate value. \DIFdelbegin \DIFdel{A candidate is randomly drawn }\DIFdelend \DIFaddbegin \DIFadd{As a result, all candidates characterized by rare tokens are grouped within a single bin. By randomly selecting candidates }\DIFaddend from each bin\DIFdelbegin \DIFdel{and analysed as an in-depth case study. The analysis is stopped when the time resource is exhausted}\DIFdelend \DIFaddbegin \DIFadd{, we ensure that the final population reflects a diverse mix of candidates, mitigating the risk of overrepresenting those that are trivial yet rare}\DIFaddend .

%DIF >  NOTE: Cohen's kappa: 0.8083955317337074
\DIFaddbegin 

\DIFaddend During each case study, \DIFdelbegin \DIFdel{we analyse the }\DIFdelend \DIFaddbegin \DIFadd{the first author categorized each candidate using }\emph{\DIFadd{inductive coding}}\DIFadd{. The first author analyzed the Python }\DIFaddend code of the candidate to understand its purpose. Additionally, \DIFdelbegin \DIFdel{we analyse }\DIFdelend the entire code cell, the previous cell, next cell and the notebook's purpose \DIFaddbegin \DIFadd{was analyzed }\DIFaddend to bring in rich context. For case studies where the first author was unable to determine the purpose of the candidate, it was additionally \DIFdelbegin \DIFdel{analysed }\DIFdelend \DIFaddbegin \DIFadd{analyzed }\DIFaddend by the second and third authors. \DIFdelbegin \DIFdel{Further discussions were held with all authors until a consensus was reached}\DIFdelend \DIFaddbegin \DIFadd{Finally, the entire research team held regular meetings throughout this iterative process to ensure consistency in the categorization of the feedback mechanisms}\DIFaddend .

\DIFaddbegin \DIFadd{A taxonomy of feedback mechanisms observed in ML Jupyter notebooks was created using the categories generated during the exploratory case study analysis. To validate the taxonomy developed by the first author, the second author independently annotated the candidates using the categories developed by the first author. The inter-rater agreement was calculated using the }\emph{\DIFadd{Cohen's Kappa}} \DIFadd{method and a score 0.8 was obtained which is regarded as highly agreeable.
}

\DIFaddend \section{Results}

%DIF >  TODO: now that we created a taxonomy, read other papers that did the same to understand how results are presented
\begin{figure}
	\centering
	\includegraphics[width=0.75\textwidth]{taxonomy.pdf}
	\caption{Overview of various feedback mechanism groups identified in this study.}
	\label{fig:taxonomy}
\end{figure}

\DIFdelbegin \DIFdel{The results of the study are presented in this section. For the case studies conducted for RQ2 and RQ3, we group the candidates based on their similarity to each other. This is done only to structure the paper in a meaningful manner and for ease of reporting the results.
}\DIFdelend Figure~\ref{fig:taxonomy} provides an overview of the various feedback mechanism groups identified in this study with additional details provided in Section~\ref{sec:result-explicit} and Section~\ref{sec:result-implicit}. Throughout the text, we reference the candidate \textbf{(A)}ssertions, \textbf{(P)}rint statements and \textbf{(L)}ast cell statements using their unique identifiers as examples. These can be viewed on our online appendix\footnote{https://arumoy.me/shome2023notebook}.

\DIFaddbegin \DIFadd{Due to the exploratory nature of this study and the limited number of case studies analyzed, the observations presented should be interpreted with caution and viewed as illustrative rather than definitive. The case studies serve to illustrate key themes, and no definitive conclusions should be drawn from individual instances.
}

\subsection{\DIFadd{Descriptive Statistics}}

%DIF >  TODO: improve the arrangement of these figures; they are taking up too much space right now.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{notebook-size-distribution.pdf}
	\caption{\DIFaddFL{Number of notebooks in the small, medium and large notebook size categories.}}
	\label{fig:notebook-size-distribution}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.75\textwidth]{cell-per-notebook-size-distribution.pdf}
	\caption{\DIFaddFL{Distribution of number of cells in all notebook size categories.}}
	\label{fig:cell-per-notebook-size-distribution}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{fms-per-notebook-size-distribution.pdf}
	\caption{\DIFaddFL{Distribution of assertions, prints and last cell statements across all notebook size categories.}}
	\label{fig:fms-per-notebook-size-distribution}
\end{figure}

\DIFadd{Figure~\ref{fig:notebook-size-distribution} shows the distribution of the number of code cells for all notebooks analyzed in this study. The distribution is grouped into three categories---namely small, medium and large---based on the first and third quartiles. This is done since the data is large, non-uniform and contains many outliers. Figure~\ref{fig:cell-per-notebook-size-distribution} further magnifies the distribution of the number of cells within each category. The dataset mostly contains medium size notebooks (9 to 26 code cells), followed by small (1 to 8 code cells) and large (27 to 772 code cells) size notebooks.
}

\DIFadd{Figure~\ref{fig:fms-per-notebook-size-distribution} shows the distribution of assertions, prints and last cell statements across the different notebook size categories. A logarithmic scale is used for the count since the data is large and contains outliers. Overall, the visualization suggests an increase in the use of all feedback mechanisms as notebook size grows. Print statements are the most frequently used across all notebook sizes.
}

\DIFaddend \subsection{(RQ1) What feedback mechanisms are employed in the development of ML Jupyter notebooks?}~\label{sec:result-analysis}

25 thousand (8.5\%) notebooks contained at least one assertion. From the 8.5\% notebooks, we collected 89.6 thousand assertions. 4.8 thousand assertions (5\%) used methods from external testing libraries. 85 thousand assertions (95\%) however were written using the \lstinline{assert} statement provided by the built-in Python standard library. We removed duplicate assertions for the remainder of the analysis, thus resulting in 27.1 thousand assertions.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{other-test-methods.pdf}
	\caption{10 most common methods used in assertions written using external testing libraries.}
	\label{fig:other-test-methods}
\end{figure}

Figure~\ref{fig:other-test-methods} presents the 10 most common methods used in the 5\% assertions written using external testing libraries. We manually tracked the source of the methods to the numpy, pandas, torch and unittest libraries. The most frequently used methods include \lstinline{assert_almost_equal} and \lstinline{assert_equal}, each with over 250 instances. Methods such as \lstinline{assert_almost_equal}, \lstinline{assert_array_almost_equal} and \lstinline{AssertAlmostEqual} accommodate the stochastic nature of ML, where results can vary slightly due to inherent randomness in data processing and model training. We also find methods such as \lstinline{assert_array_equal} and \lstinline{assert_frame_equal} that are specifically tailored for comparing high-dimensional data structures such as numpy arrays and pandas dataframes.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 1.1} 25 thousand (8.5\%) notebooks contained at least one assertion. 95\% of the 89.6 thousand assertions were written using the built-in Python \lstinline{assert} statement and 5\% using external testing libraries. Methods such as \lstinline{assert_almost_equal} and \lstinline{assert_array_equal} from the numpy and pandas libraries were frequently used to accomodate for the stochastic nature of ML algorithms and comparing high-dimensional data structures.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 1.1} 25 thousand (8.5\%) notebooks contained at least one assertion. 95\% of the 89.6 thousand assertions were written using the built-in Python {\color{blue}%DIFAUXCMD
\lstinline{assert} %
}%DIFAUXCMD
statement and 5\% using external testing libraries. Methods such as {\color{blue}%DIFAUXCMD
\lstinline{assert_almost_equal} %
}%DIFAUXCMD
and {\color{blue}%DIFAUXCMD
\lstinline{assert_array_equal} %
}%DIFAUXCMD
from the numpy and pandas libraries were frequently used to accommodate for the stochastic nature of ML algorithms and to compare high-dimensional data structures.}
\DIFaddend 

\begin{figure}
	\centering
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-assert-test.pdf}
		\caption{10 most common AST nodes in the test attribute of \lstinline{assert} statements.}
		\label{fig:common-assert-test}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-compare-op.pdf}
		\caption{10 most common operators used in \lstinline{assert} statements that perform a comparison check.}
		\label{fig:common-compare-op}
	\end{minipage}
\end{figure}

\begin{table}
	\centering
	\caption{Examples of the most common tests performed in \lstinline{assert} statements.}
	\begin{tabular}{@{}m{0.5\textwidth} m{0.5\textwidth}@{}}
		\toprule
		\emph{\textbf{Comparison Check}}              &
		\emph{\textbf{Function Call}}                             \\
		\midrule

		\lstinline[]$type(train_dataset[0]) == tuple$ &
		\lstinline[]$(np.unique(y_test) == [-1, 1]).all()$        \\

		\lstinline[]$torch.__version__ > '1.10.0'$    &
		\lstinline[]$isinstance(transmat, pd.dataframe)$          \\

		\lstinline[]$len(x_dev) == len(y_dev)$        &
		\lstinline[]$np.allclose(np.linalg.norm(wio, axis=0), 1)$ \\
		\bottomrule
	\end{tabular}
	\label{tab:common-assert-test}
\end{table}

We focus the remainder of the analysis on the 95\% of the assertions written using the \lstinline{assert} statement. Figure~\ref{fig:common-assert-test} shows the most frequent AST nodes that occur in \lstinline{assert} statements. The figure indicates that the most common test performed in the \lstinline{assert} statements include a comparison check and a function call. Table~\ref{tab:common-assert-test} provides a few examples of assertions for each of the tests above. In Figure~\ref{fig:common-compare-op} we see the most common operators used in assert statements that perform a comparison check. We observe that overwhelming majority of the asserts ensure that two values are equal to each other. Figure~\ref{fig:common-compare-lhs} and Figure~\ref{fig:common-compare-rhs} show the most common bi-grams that appear in the LHS and RHS of the comparison statements respectively. The figures collectively show that the shape and size of data structures are frequently checked.

\begin{figure}
	\centering
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-compare-lhs.pdf}
		\caption{Most common bi-grams that appear in the LHS of comparison statements.}
		\label{fig:common-compare-lhs}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-compare-rhs.pdf}
		\caption{Most common bi-grams that appear in the RHS of comparison statements.}
		\label{fig:common-compare-rhs}
	\end{minipage}
\end{figure}

Of the 95\% assert statements, 28.4 thousand (31.7\%) have a failure message. Figure~\ref{fig:common-assert-msgs} shows the most frequent bi-grams that appear in these failure messages. Similar to the analysis presented above, we removed the duplicate messages prior to \DIFdelbegin \DIFdel{analysing }\DIFdelend \DIFaddbegin \DIFadd{analyzing }\DIFaddend the bi-grams resulting in 5.9 thousand unique messages. The bi-grams indicate that the shape and data type of data structures are frequently verified.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 1.2} 31.7\% of the assertions written using the built-in \lstinline{assert} statement have a failure message. These assertions frequently perform a comparison check or call an external function. The comparison checks frequently verify that the shape and size of data structures is equal to a specified value.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 1.2} 31.7\% of the assertions written using the built-in {\color{blue}%DIFAUXCMD
\lstinline{assert} %
}%DIFAUXCMD
statement have a failure message. These assertions frequently perform a comparison check or call an external function. The comparison checks frequently verify that the shape and size of data structures is equal to a specified value.}
\DIFaddend 

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{common-assert-msgs.pdf}
	\caption{Most common bi-grams in the failure messages of assert statements.}
	\label{fig:common-assert-msgs}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{common-print-nodes.pdf}
		\caption{Most common AST nodes inside print statements}
		\label{fig:common-print-nodes}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{common-print-constant-types.pdf}
		\caption{Most common type of constant inside print statements}
		\label{fig:common-print-constant-types}
	\end{minipage}
\end{figure}

We collected 1.4 million print statements from 180 thousand notebooks (60\% of the total notebooks \DIFdelbegin \DIFdel{analysed }\DIFdelend \DIFaddbegin \DIFadd{analyzed }\DIFaddend in this study). Unlike assert statements, print statements do not have a fixed lexical semantic since the Python \lstinline{ast} module interprets them as regular function calls. Additionally, print statements can be written in several forms---the \lstinline{print} function may have multiple arguments, any form of string interpolation supported by Python or output from function calls. Figure~\ref{fig:common-print-nodes} shows that the most common AST nodes inside \DIFdelbegin \DIFdel{of }\DIFdelend print statements are constants followed by function calls. Figure~\ref{fig:common-print-constant-types} further validates that the constants are almost entirely of type string. Based on these observations, we dissect the print statements into two components---text written in natural language (from the \emph{Constant} nodes) and code (from all other nodes). Figure~\ref{fig:common-print-constants} and Figure~\ref{fig:common-print-not-constants} present the most common bi-grams that appear in the natural text and code of print statements respectively. Both figures collectively indicate that print statements are frequently used to print various performance metrics of ML models.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 1.3} Print statements are most commonly used to display various performance metrics of ML models.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 1.3} Print statements are most commonly used to display various performance metrics of ML models.}
\DIFaddend 

\begin{figure}
	\centering
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-print-constants.pdf}
		\caption{Most common bi-grams that appear in natural text of print statements.}
		\label{fig:common-print-constants}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-print-not-constants.pdf}
		\caption{Most common bi-grams that appear in the code of print statements.}
		\label{fig:common-print-not-constants}
	\end{minipage}
\end{figure}

We collected 1 million last cell statements from 119 thousand notebooks (40\% of the total notebooks \DIFdelbegin \DIFdel{analysed }\DIFdelend \DIFaddbegin \DIFadd{analyzed }\DIFaddend in this study). Figure~\ref{fig:common-last-nodes} shows that \emph{Call} nodes frequently occur in the AST of last cell statements. Figure~\ref{fig:common-last-modules} shows the most common Python modules and objects while Figure~\ref{fig:common-last-functions} \DIFdelbegin \DIFdel{show }\DIFdelend \DIFaddbegin \DIFadd{shows }\DIFaddend the most common functions used inside these \emph{Call} nodes. The figures highlight the prominence of data visualization libraries seaborn (\lstinline{sns}) and matplotlib (\lstinline{plt}) and visualization functions such as \lstinline{plot} and \lstinline{countplot} in the notebooks. The data analysis tool pandas (represented by \lstinline{df} and \lstinline{pd}) also features prominently with many data exploration and preprocessing functions such as \lstinline{head}, \lstinline{value_counts} and \lstinline{describe} being used frequently. The presence of \lstinline{train} and \lstinline{model} Python objects along with the \lstinline{fit} function indicates the use of ML libraries.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 1.4} Libraries and functions for data visualization, data exploration and machine learning are most common in last cell statements.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 1.4} Libraries and functions for data visualization, data exploration and machine learning are most common in last cell statements.}
\DIFaddend 

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{common-last-nodes.pdf}
	\caption{Most common AST nodes in the last cell statements.}
	\label{fig:common-last-nodes}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-last-modules.pdf}
		\caption{Most common Python modules and objects used in the \emph{Call} nodes of last cell statements.}
		\label{fig:common-last-modules}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-last-functions.pdf}
		\caption{Most common functions used in the \emph{Call} nodes of last cell statements.}
		\label{fig:common-last-functions}
	\end{minipage}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-last-df-functions.pdf}
		\caption{Most common functions called on pandas dataframes.}
		\label{fig:common-last-df-functions}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{common-last-visualization-functions.pdf}
		\caption{Most common visualization functions used in last statements.}
		\label{fig:common-last-visualization-functions}
	\end{minipage}
\end{figure}

We further \DIFdelbegin \DIFdel{analysed }\DIFdelend \DIFaddbegin \DIFadd{analyzed }\DIFaddend the most common functions called on pandas dataframes by isolating the last cell statements that contain \lstinline{data} or \lstinline{df} objects. This is shown in Figure~\ref{fig:common-last-df-functions} which reveals a focus on exploratory data analysis and statistical operations. The frequent use of functions such as \lstinline{value_counts}, \lstinline{head}, and \lstinline{unique}, indicate an emphasis on understanding the structure and content of the data. Visualization functions like \lstinline{plot} and \lstinline{hist} suggest that practitioners often create quick visualizations directly from dataframes. Descriptive statistics are also prominent, with functions like \lstinline{describe}, \lstinline{mean}, and \lstinline{sum} being commonly used.

Similarly, we also \DIFdelbegin \DIFdel{analysed }\DIFdelend \DIFaddbegin \DIFadd{analyzed }\DIFaddend the most common visualization functions used in notebooks by isolating the last cell statements where the module is \lstinline{sns} or \lstinline{plt}. This is shown in Figure~\ref{fig:common-last-visualization-functions} which indicates that categorical data visualization appears to be a primary focus, with \lstinline{countplot} and \lstinline{barplot} ranking high among the most frequently used functions. The prominence of \lstinline{heatmap} and \lstinline{boxplot} functions indicates a regular need for visualizing correlations and statistical distributions. For continuous data, functions like \lstinline{distplot}, \lstinline{scatterplot}, and \lstinline{scatter} are commonly used. The presence of \lstinline{imshow} suggests that image-related visualizations are also significant in many notebooks.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 1.5} Common functions called on pandas dataframes focus on exploratory data analysis, statistical operations and quick visualizations. In visualizations, functions for categorical data, correlation and distribution are frequently used.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 1.5} Common functions called on pandas dataframes focus on exploratory data analysis, statistical operations and quick visualizations. In visualizations, functions for categorical data, correlation and distribution are frequently used.}
\DIFaddend 

\subsection{(RQ2) How is explicit feedback from assert statements used to validate ML code written in Jupyter notebooks?}~\label{sec:result-explicit}

\subsubsection{Data shape check ($N = 26$)}

Data shape checks can be considered the ``Swiss army knife'' of assertions, as they are ubiquitous and serve multiple purposes throughout the MLDL. We encountered a vast array of assert statements that verify the dimensions of various data structures, including input features (\emph{A17, A90}), labels (\emph{A5, A29}), predictions, images (\emph{A76, A93}), sequences, and embeddings (\emph{A76, A93}). These assertions ensure that the data adheres to the expected format and dimensions required by different components of the machine learning pipeline.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.1} 26 assertions verify the dimensions of various data structures such as input features, labels, predictions, images, sequences, and text embeddings. These checks ensure data consistency and compatibility throughout the ML pipeline, preventing runtime errors and ensuring accurate model training and evaluation.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.1} 26 assertions verify the dimensions of various data structures to ensure data consistency and compatibility throughout the ML pipeline, prevent runtime errors and ensure accurate model training and evaluation.}
\DIFaddend 

\subsubsection{Data validation check ($N = 14$)}

We found assert statements that perform various validation checks on data structures, such as NumPy arrays and Pandas dataframes. Common data validation checks include verifying the presence of specific values or ranges within arrays or columns(\emph{A41, A65}) and verifying the equality or closeness of arrays to a target value within a specified absolute tolerance (\emph{A44, A46}). Some assertions also focus on validating the uniqueness or cardinality of values (\emph{A52}) while others validate the presence of specific values or conditions within data structures (\emph{A73}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.2} Fourteen assertions perform validation checks on data structures, ensuring the presence of specific values, verifying value ranges, and confirming uniqueness or cardinality of values. These checks catch potential errors or inconsistencies early, ensuring data meets specific criteria or constraints for robust ML applications.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.2} Fourteen assertions perform validation checks on data structures to catch errors or inconsistencies early, ensuring data meets specific criteria or constraints for robust ML applications.}
\DIFaddend 

\subsubsection{Model performance check ($N = 11$)}\label{sec:assert-model-perf}

\begin{lstlisting}[caption={Assertion \emph{A58} used to check that the accuracy of a model is close to the specified value. The use of \lstinline{np.isclose} allows for small deviations in the accuracy thus accounting for the stochastic nature of ML models.}, label={lst:A58}]
assert np.isclose(accuracy, 0.9666666666666667)
\end{lstlisting}

This study finds several assertions used to test model performance against predefined thresholds for key metrics such as accuracy, precision, recall and F1 score. Range checks confirm that the performance metrics fall within acceptable boundaries, indicating good predictive performance without overfitting (\emph{A38}). Direct comparisons of model outputs with expected results, validate the model's predictive accuracy and reliability under operational conditions (\emph{A15}). Other assertions ensure that the learned parameters align closely with theoretically or empirically derived values, further cementing the model’s statistical validity (\emph{A19}). Finally, assertions are also used to check for the expected number of \DIFdelbegin \DIFdel{neighbours }\DIFdelend \DIFaddbegin \DIFadd{neighbors }\DIFaddend as an indirect measure of model performance in specific scenarios like clustering (\emph{A7}).

The use of \lstinline{np.isclose} in Listing~\ref{lst:A58} is particularly noteworthy. This method accounts for the stochastic nature of many ML models, where slight variations in performance metrics can occur due to differences in initial conditions, random seed settings, or inherent randomness in algorithms. By allowing a small tolerance in the comparison, \lstinline{np.isclose} ensures that the model's performance is consistently close to the expected benchmark despite the stochastic elements.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.3} Eleven assertions evaluate model performance against predefined thresholds for accuracy, precision, recall, F1 score, and other metrics. These checks validate the model's predictive accuracy and reliability, ensuring it meets expected performance standards despite the stochastic nature of ML models.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.3} Eleven assertions evaluate model performance against predefined thresholds for accuracy, precision, recall, F1 score, and other metrics. These checks validate the model's predictive accuracy and reliability, ensuring it meets expected performance standards despite the stochastic nature of ML models.}
\DIFaddend 

\subsubsection{Existence check ($N = 8$)}

Existence checks are primarily used during the data preprocessing stage in the MLDL to ensure data integrity before analysis and modelling. These checks primarily focus on verifying the presence of necessary columns and the absence of missing values within those columns (\emph{A23, A42, A79, A86}). Such validations are especially crucial after data preprocessing steps, where transformations might inadvertently introduce \lstinline{NaN} values (\emph{A50, A51, A63}) or result in an empty dataframe (\emph{A43}). These checks are also crucial for maintaining the accuracy and efficiency of data handling processes, ensuring that the datasets are ready for robust ML applications.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.4} Eight assertions ensure the presence of specific columns in the dataset and the absence of missing values. These checks prevent operations on empty datasets and confirm the absence of \texttt{NaN} values, maintaining data integrity and reliability for model training.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.4} Eight assertions ensure the presence of specific columns in the dataset and the absence of \texttt{NaN} or missing values. These checks prevent operations on empty datasets to maintain data integrity and reliability for model training.}
\DIFaddend 

\subsubsection{Resource check ($N = 7$)}~\label{sec:result-rq2-resource-check}

\begin{lstlisting}[caption={Assertion \emph{A37} used to ensure that an ML model has not reached an inconsistent state due to out-of-order or re-execution of code cells.}, label={lst:A37}]
assert svm.fit_status_ == 0, 'Forgot to train the SVM!'
\end{lstlisting}

Resource check assertions are used to ensure the availability and validity of essential resources, preventing runtime errors, and maintaining the integrity of the data, models, and visualizations. One set of assertions verify the existence of files on the file system, such as pre-trained models (\emph{A10}) or data files (\emph{A74}). Subsequently, assertions are employed to validate the successful loading of models (\emph{A14}) and the completion of model training processes (\emph{A37}).

A unique aspect of working with notebooks is the ability to re-run cells while experimenting, which can lead to unintended consequences. Assertion \emph{A37} presented in Listing~\ref{lst:A37} addresses this scenario by checking if the SVM model has been properly trained before proceeding with further operations. This check prevents inconsistent model states from executing cells out of order or multiple times. Other interesting assertions unique to computational notebooks include verifying the presence of data in visualizations (\emph{A60}) and explicitly verifying the versions of critical libraries (\emph{A18, A67}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.5} Seven assertions confirm the availability of essential resources like pre-trained models, data files, successful model loading, and completion of training processes. These checks prevent runtime errors and ensure proper code execution, maintaining the reliability and robustness of the ML workflows.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.5} Seven assertions confirm the availability of essential resources like pre-trained models, data files, successful model loading, and completion of training processes. These checks prevent runtime errors and ensure proper code execution, maintaining the reliability and robustness of the ML workflows.}
\DIFaddend 

\subsubsection{Type check ($N = 5$)}

Type check assertions are used to maintain data integrity, ensure compatibility between different components of the ML pipeline, and preventing errors and unexpected \DIFdelbegin \DIFdel{behaviour}\DIFdelend \DIFaddbegin \DIFadd{behavior}\DIFaddend . Type checks are commonly performed to ensure that the input features are PyTorch float tensors or NumPy arrays, a requirement for many deep learning models and scientific computing operations respectively (\emph{A2, A88}). Type checks are also applied to verify the type of the ML models (\emph{A40}), data structures and intermediate objects (\emph{A35}) and individual columns or elements within a dataset (\emph{A81}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.6} Five assertions validate the data types of input features, models, and intermediate objects. These checks ensure compatibility with ML models and operations, preventing errors from incompatible methods or attributes, thus maintaining the reliability of the ML workflow.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.6} Five assertions validate the data types of input features and Python objects. These checks prevent errors from incompatible methods or attributes, and improve the reliability of the ML workflow.}
\DIFaddend 

\subsubsection{Mathematical property check ($N = 4$)}

When working with neural networks and other statistical models, it is imperative to ensure that mathematical properties are preserved throughout operations involving arrays and matrices. This \DIFdelbegin \DIFdel{rigour }\DIFdelend \DIFaddbegin \DIFadd{rigor }\DIFaddend is captured though assertions that ensure convolution operations are dimensionally consistent (\emph{A3}), checking for the symmetry of matrices (\emph{A64}) and monitoring the standard deviation of outputs to decide the appropriateness of using batch normalization (\emph{A25}). These assertions are not just precautionary but are vital for confirming that the underpinnings of ML algorithms align with expected mathematical principles.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.7} Four assertions validate key mathematical properties of data and model outputs. These include ensuring dimensional consistency in convolution operations, monitoring standard deviation for batch normalization suitability, verifying uniformity in computations, and checking matrix symmetry. These checks confirm the robustness and reliability of model computations.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.7} Four assertions validate key mathematical properties of data and model outputs to confirm the robustness and reliability of model computations.}
\DIFaddend 

\subsubsection{Batch size check ($N = 3$)}

In the context of neural networks, the practice of using batch processing is a key strategy to enhance computational efficiency and hardware utilization. Batching stabilizes the learning curve by updating the model weights based on the average gradient of a batch thereby optimizing learning and enhancing generalizability of the model. Ensuring the batch size matches the hardware capacity is crucial to avoid memory overflow issues that can halt training. Assertions are used to check that the batch size divides evenly into the training dataset size which ensures that each training epoch is computationally efficient (\emph{A21}). Additionally, assertions are used to ensure that the size of the training data is larger than the batch size (\emph{A70}) and that image dimensions are suitably divisible by the patch size which is essential for certain convolutional networks (\emph{A28}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.8} This study identifies three assertions validating batch size configurations. These assertions ensure batch sizes are divisible by dataset sizes, image dimensions are appropriately divided by patch sizes, and the number of images exceeds the batch size. These checks are crucial for optimizing computational efficiency and ensuring stable and smooth model training.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.8} Three assertions validate the batch size configurations. Such checks are crucial for optimizing computational efficiency and ensuring stable and smooth model training.}
\DIFaddend 

\subsubsection{Network architecture check ($N = 3$)}

When integrating pre-trained models or leveraging transfer learning techniques, it is crucial to ensure the compatibility of the custom neural network architecture with the pre-trained components. This is particularly important when combining convolutional layers from different sources, as the spatial dimensions and channel configurations must align correctly. Checks include assertions that verify the output and input dimensions of consecutive convolutional layers match (\emph{A11}), checking that shape of the activations have the right number of channels (\emph{A62}) and that the appropriate regularization parameter is set (\emph{A75}). Such architectural checks are crucial for preventing potential issues during the forward pass of the neural network and ensuring that the custom and pre-trained components are compatible with each other.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.9} Three assertions verify the consistency of neural network architecture. These include ensuring weight dimensions match across layers and that activation shapes and regularization parameters are as expected. These checks ensure that the neural network is correctly configured and integrated with pre-trained models, maintaining the integrity of data flow during training.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.9} Three assertions verify that the neural network is correctly configured and integrated with pre-trained models to maintain the integrity of data flow during training.}
\DIFaddend 

\subsubsection{Data leakage check ($N = 1$)}

\begin{lstlisting}[caption={Assertion \emph{A33} used to ensure that the training and validation sets do not contain any overlapping values.}, label={lst:A33}]
assert len(
  set(tr_df.PetID.unique()).intersection(valid_df.PetID.unique())
) == 0
\end{lstlisting}

Ensuring the absence of data leakage between training and validation datasets is a fundamental best practice in ML to prevent overfitting. To guarantee that the model can generalize effectively to new examples, it is crucial to verify that the training and validation sets are completely distinct with no shared examples. By strictly separating these datasets, the validation phase provides an unbiased evaluation of the model's ability to generalize to data similar to, but not identical to that which it was trained on. This is demonstrated by Assertion \emph{A33} in Listing~\ref{lst:A33} which ensures that there are no overlapping \lstinline{PetID} values between the training set and the validation set, thereby preventing any possibility of data leakage.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 2.10} This study identifies one assertion to ensure there are no overlapping values between training and validation sets to prevent data leakage. Preventing data leakage is essential to avoid overfitting and ensure the model's ability to generalize to new data. This practice maintains the integrity and validity of the model evaluation metrics.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 2.10} One assertion ensures there are no overlapping values between the training and validation sets to prevent data leakage and subsequently overfitting during model training.}
\DIFaddend 

\subsection{(RQ3) How is implicit feedback from print statements and last cell statements used when writing ML code in Jupyter notebooks?}~\label{sec:result-implicit}

\subsubsection{Model performance check ($N = 33$)}

During the model development phase, the performance of an ML model is often printed to facilitate comparisons against other models or variations. Throughout this continuous experimentation phase, authors may adjust model parameters or modify the data by engineering new features. The model is then re-trained to check if these changes lead to performance improvements. Besides the \emph{accuracy} of the model (\emph{P3, P18}), we also see checks for the \emph{Root Mean Square Error (RMSE)} (\emph{P6}) and the use of the classification report (\emph{P50}) provided by scikit-learn\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification\_report.html} which reports the \emph{accuracy}, \emph{precision}, \emph{recall} and \emph{f1 score} for all labels present in the target feature. In addition, we find the use of heatmaps (\emph{L3}) to visualize the confusion matrix of a multi-label classification task and custom functions (\emph{L52}) to manually evaluate an image classification model on a random sample of input images.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.1} The study identifies 33 implicit checks to monitor the performance of trained ML models. These include outputs that print key performance metrics, a normalized heatmap used to visualize the confusion matrix of a model's predictions and a custom function written for manual evaluation of model predictions on random input samples. These checks are used throughout the iterative model development phase, enabling practitioners to compare different models or variations, adjust parameters, and refine features to improve performance.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.1} 33 implicit checks are used to monitor the performance of trained ML models. These checks are used throughout the iterative model development phase to compare different models or variations, adjust parameters, and refine features to improve performance.}
\DIFaddend 

\subsubsection{Data distribution check ($N = 7$)}~\label{sec:data-distribution-output}

Understanding the distribution of data is crucial for making informed decisions about necessary transformation steps in data analysis. For instance, visualizations can efficiently determine if scaling, normalizing, or handling outliers is needed. During the exploratory data analysis phase, visualizations and pandas dataframes are commonly used to assess the distribution of specific columns (\emph{L9}), helping identify features related to the target variable (\emph{L2, L14}) and informing feature inclusion in model training. Additionally, descriptive statistics are used post-transformation to manually verify the effectiveness of data transformation steps and ensuring that the data conforms to the expected format for further analysis (\emph{L48, L25}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.2} The study identifies seven implicit data distribution checks, including visualizations like categorical plots, kernel density estimate plots and count plots, and statistical summaries from pandas dataframes. These checks are used for making informed decisions about data transformations such as scaling, normalizing and handling outliers, ensuring data integrity and improving model training reliability during the exploratory data analysis phase.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.2} Seven implicit data distribution checks are used to make informed decisions about data transformations, ensuring data integrity and improving model training reliability during the exploratory data analysis phase.}
\DIFaddend 

\subsubsection{Resource check ($N = 7$)}\label{sec:implicit-resource-check}

Similar to the assertions identified in Section~\ref{sec:result-rq2-resource-check}, we also find print and last cell statements that validate the availability of essential resources on the system where the notebooks are being executed. This includes checks for the availability of compute resources such as a GPU or a TPU (\emph{P68, P82, P86}), verifying that a dataset or a pre-trained model has been successfully loaded into memory (\emph{P107, L66}) and ensuring that a certain version of an external library is present on the system (\emph{P71}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.3} The study identifies seven implicit checks used to verify resource availability on the systems where the Jupyter notebooks are being executed. These checks include confirming the availability of compute resources like GPUs and TPUs, ensuring datasets or pre-trained models are successfully loaded, and verifying the presence of specific library versions. These checks are used for ensuring that the computational environment is correctly set up, preventing execution errors, and facilitating smooth workflow execution.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.3} Seven implicit checks are used to verify critical resources are available on the system. These checks can help ensure that the computational environment is correctly setup and prevent execution errors.}
\DIFaddend 

\subsubsection{Spot check ($N = 5$)}

Performing value or spot checks can be an essential practice for ensuring data integrity and model accuracy at various stages of development. These checks are crucial for verifying that operations such as data transformations, model predictions, and feature engineering are functioning as expected. Common checks include verifying the number of features in the data matches expectations after applying feature selection or dimensionality reduction techniques (\emph{L60}), assessing the correctness of one-hot encoding (\emph{P114}) and ensuring that data retains its expected properties after manipulation (\emph{P64}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.4} This study finds five implicit spot checks performed at various stages of the MLDL. Spot checks are used to ensure data integrity by verifying data transformation and feature engineering steps meet expectations.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.4} Five implicit spot checks are performed throughout the various stages of the MLDL to verify data transformation and feature engineering steps meet expectations.}
\DIFaddend 

\subsubsection{Model training check ($N = 4$)}

During training, it is common practice to monitor the progress periodically and use this feedback to adjustment training parameters or stop training early to prevent overfitting. This can be done by checking the best parameters found through tuning methods (\emph{L42}), fitting the model to the training data (\emph{L31}) or printing the training loss and accuracy to give real-time feedback on the learning efficacy of the model (\emph{L8}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.5} The study identifies four implicit checks used to monitor model training progress. These checks provide real-time feedback on training loss and accuracy, validate model adherence to expected data patterns, and identify optimal parameters through tuning methods. These checks facilitate timely adjustments during the training process to prevent overfitting and ensure model performance optimization.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.5} Four implicit checks are used to monitor the model training progress. These checks facilitate timely adjustments during the training process to prevent overfitting and ensure model performance optimization.}
\DIFaddend 

\subsubsection{Missing value check ($N = 3$)}

Checking for missing values is a critical step in the preprocessing phase of an ML project because it significantly influences the quality and performance of the model. Missing data can lead to biased or incorrect conclusions if not handled properly, potentially skewing the model's performance by training on incomplete or non-representative samples~\citep{shome2022data}. Many ML algorithms demand complete numerical datasets to perform calculations such as matrix multiplication. Missing values interrupt these calculations, leading to errors or the inability to execute algorithms entirely. These checks can be performed in code (\emph{P74, L36}) and using visualizations such as a heatmap (\emph{L12}).

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.6} The study identifies three implicit checks for missing data. Missing values can lead to biased or incorrect conclusions as many ML algorithms require complete numerical datasets for accurate calculations. Identifying and addressing missing data during preprocessing is essential to avoid errors and improve model reliability.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.6} Three implicit checks are used to identify and address missing data during preprocessing which is essential to avoid errors and improve model reliability.}
\DIFaddend 

\subsubsection{Shape check ($N = 3$)}

Ensuring that data dimensions align with expectations is important particularly following data pre-processing or transformation steps. Primarily, the number of features in the training set must match those in the testing set (\emph{P4, P32, P117}). This alignment is crucial because statistical ML models are trained on specific data dimensions and expect the same dimensional structure during inference to perform accurately. Similarly, in neural network architectures, the configuration of input layers depends directly on the shape of the training data, dictating the number of input neurons needed. Furthermore, the correspondence between the number of test examples and their respective labels is essential for accurately computing performance metrics.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.7} The study identifies three implicit checks used to validate the shape of data. These checks are used to ensure data dimensions align with expectations, particularly after preprocessing or transformation steps.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.7} Three implicit checks are used to validate the shape of data. These checks are used to ensure data dimensions align with expectations, particularly after preprocessing or transformation steps.}
\DIFaddend 

\subsubsection{Data relationship check ($N = 2$)}~\label{sec:linear-relation-output}

\begin{table}
	\centering
	\caption{Last cell statements used to verify linear relationship between features in the data.}
	\begin{tabular}{@{}m{0.05\textwidth} m{0.4\textwidth} m{0.4\textwidth}@{}}
		\toprule
		\emph{\textbf{Key}}                                                                                                   &
		\emph{\textbf{Code}}                                                                                                  &
		\emph{\textbf{Output}}                                                                                                  \\
		\midrule

		L6                                                                                                                    &
		\lstinline[]$b = sns.relplot(x='SIZE', y='Cash', hue='CLARITY', alpha=0.9, palette='muted', height=8, data=raw_data)$ &
		\includegraphics[width=\linewidth]{linear-relation-check-lineplot.png}                                                  \\

		L10                                                                                                                   &
		\lstinline[]$sns.regplot(x='X4 number of convenience stores', y='Y house price of unit area', data=data)$             &
		\includegraphics[width=\linewidth]{linear-relation-check-regplot.png}                                                   \\
		\bottomrule
	\end{tabular}
	\label{tab:linear-relation-check}
\end{table}

Linear ML models achieve optimal performance when the target variable can be expressed as a linear combination of the input features. However, features within a dataset that exhibit a linear relationship are considered redundant as they convey the same information to the model during training. Consequently, the feature engineering stage often involves removing such features to create a more efficient training dataset~\citep{shome2022data}. Table~\ref{tab:linear-relation-check} illustrates two last cell statements found in this study for identifying linear relationships among features in the data. In addition to the code, the raw output of the cell is also shown since both case-studies produce a visualization.  In \emph{L6}, the practitioner assesses the linearity between the features \texttt{Cash} and \texttt{SIZE}. \emph{L10} is a visualization of the feature \texttt{X4} alongside the target variable \texttt{Y}, accompanied by a fitted linear regression model.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.8} The study identifies two last cell statements used to verify linear relationships between features in the data. Visualizations such as a scatter plot with a regression line and a line plot are employed to assess the linearity between features. These checks are used for identifying and potentially removing redundant features or performing feature selection to ensure that the training dataset is optimized for linear ML models.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.8} Two last cell statements that produce visualizations are used to verify linear relationships between features in the data. Such checks help identify and remove redundant features ensure that the training dataset is optimized for linear ML models.}
\DIFaddend 

\subsubsection{Type check ($N = 2$)}

Type checks are crucial not only for confirming the current state of the data but also for determining the specific transformations required to make the data suitable for subsequent modelling steps. Ensuring data types align with model requirements is a fundamental step in the exploratory data analysis phase. Since all ML algorithms perform mathematical operations, they require the input data to be entirely numerical to avoid computational errors (\emph{P43, L71}). The process of checking data types helps in identifying the necessary pre-processing or transformation steps to convert the data into a form that is compatible with these models. For example, numerical features often require normalization to bring them within a specific scale and categorical features must be converted into a numerical format through methods like one-hot encoding to ensure they can be effectively integrated into the model.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.9} The study identifies two implicit checks used to verify the data types of features in the dataset. These checks are used to determine the necessary preprocessing or transformation steps required to make the data suitable for training. Type checks are also used to ensure that all features are numerical prior to performing mathematical operations such as matrix multiplication, which is used in several ML models.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.9} Two implicit checks are used to verify the data types of features in the dataset. These checks inform the necessary transformation steps required to make the data suitable for training. Additionally, type checks ensure that all features are numerical prior to performing mathematical operations such as matrix multiplication, which are fundamental to various ML models.}
\DIFaddend 

\subsubsection{Execution time check ($N = 1$)}
\begin{lstlisting}[caption={Print statement \emph{P66} used to check the total execution time of training an ML model with various hyper-parameter configurations.}, label={lst:P66}]
print('Total Run Time:')
\end{lstlisting}

While training multiple ML models for a single task is common, choosing the best model considers not only its performance but also its training speed. Faster training times are more cost-effective for iterative tasks and promote sustainability in the long run~\citep{shome2022data}. Listing~\ref{lst:P66} presents print statement \emph{P66} that prints the total execution time of a code cell which trains an ML model with various hyperparameter configurations using a grid-search.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.10} The study identifies one print statement used to check the total execution time of training and cross-validating an ML model. The print statement can be used to evaluate the efficiency of model training, especially when multiple models are trained iteratively.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.10} One print statement was used to check the total execution time of training and cross-validating an ML model. The print statement can be used to evaluate the efficiency of model training, especially when multiple models are trained iteratively.}
\DIFaddend 

\subsubsection{Network architecture check ($N = 1$)}

Printing the structure of a neural network (\emph{P92}) serves crucial purposes such as verifying the correct implementation of the architecture, aiding in debugging by ensuring layer compatibility, and providing a clear, human-readable format for documentation and educational insights. It helps in verifying configurations like layer dimensions and operations before initiating computationally intensive training and ensuring that the model is set up efficiently for the intended tasks.

\DIFdelbegin %DIFDELCMD < \highlight{\textbf{Finding 3.11} The study identifies one print statement used to show the architecture of a neural network. Such outputs can be used during the model development phase to ensure compatibility between the layers of a neural network, for understanding and learning about the architecture of pre-trained models and for documentation purposes.}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \highlight{\textbf{Observation 3.11} One print statement was used to check the architecture of a neural network which can facilitate validation, learning and documentation tasks.}
\DIFaddend 

\section{Discussion}\label{sec:discuss}

Drawing from the results of this study, we discuss implications and future work in this section.

%DIF >  TODO: ideas towards discussion regarding best practices
%DIF >  Observation 3.3 indicates that print statements are defined towards to beginning of the notebook (prior to any ML workflow code is executed) to ensure critical resources are available on the system where we are executing the notebook. One particular example P71 checks if a particular version of an external library is available. Perhaps we can comment on this anti-pattern, it should be defined and managed by a dependency management system such as requirements.txt to ensure a reproducible environment.
%DIF >  TODO: explicitly link the observations to the discussion points below
\DIFaddbegin 

\DIFaddend \subsection{Documentation and Technical Debt}

Jupyter notebooks are primarily used to explore different ideas or approaches, often neglecting software development best practices in \DIFdelbegin \DIFdel{favour }\DIFdelend \DIFaddbegin \DIFadd{favor }\DIFaddend of rapid experimentation~\citep{kery2018story,rule2018exploration,pimentel2019large-scale}. Once the code is finalized, it is typically transferred into a script that is run in a production environment. Alternatively, the notebook may contain significant exploratory work such as developing a model or performing data exploration. To effectively communicate the information present in the notebooks with clients, collaborators or teammates, the practitioners add annotations and structure to the notebook using markdown cells~\citep{kery2018story,rule2018exploration}. In both scenarios, practitioners need to revisit the notebooks to obtain up-to-date and accurate requirements, their rationale and design decisions, and \DIFdelbegin \DIFdel{tests---artefacts }\DIFdelend \DIFaddbegin \DIFadd{tests---artifacts }\DIFaddend which are usually missing in the original notebooks~\citep{pimentel2019large-scale,psallidas2019data,grotov2022large-scale}.

Our findings indicate that structured information can be effectively derived from the feedback mechanisms available in Jupyter notebooks. The results demonstrate that practitioners \DIFdelbegin \DIFdel{utilise }\DIFdelend \DIFaddbegin \DIFadd{utilize }\DIFaddend assertions, print statements, code cell outputs, and \DIFdelbegin \DIFdel{visualisations }\DIFdelend \DIFaddbegin \DIFadd{visualizations }\DIFaddend to inform data preprocessing techniques, detect erroneous assumptions, create new features to enhance model performance, and verify hardware and software dependencies. \DIFdelbegin \DIFdel{As data, personnel, and system environments inevitably change, undocumented decisions can lead to accumulation of technical debt. This not only prolongs time required for debugging and troubleshooting, but also incurs additional costs and resource expenditures~\mbox{%DIFAUXCMD
\citep{sculley2015hidden,amershi2019software,sambasivan2021everyone}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Our }\DIFdelend \DIFaddbegin \DIFadd{Moreover, our }\DIFaddend results from RQ3 reveal that implicit feedback from print and last cell statements are used in Jupyter notebooks to make design and implementation decisions throughout the machine learning development lifecycle. Unlike assertions, undocumented decisions based on implicit feedback mechanisms incur higher technical debt since they don't stop the execution despite violating the underlying assumptions of the code. \DIFaddbegin \DIFadd{As data, personnel, and system environments inevitably change, undocumented decisions can lead to accumulation of technical debt. This not only prolongs time required for debugging and troubleshooting, but also incurs additional costs and resource expenditures~\mbox{%DIFAUXCMD
\citep{sculley2015hidden,amershi2019software,sambasivan2021everyone}}\hskip0pt%DIFAUXCMD
.
}\DIFaddend 

We recommend that ML practitioners document insights derived from implicit feedback mechanisms, especially for visualizations. Visual cues and trends in visualizations are open to interpretation, meaning that different practitioners may interpret the same plot differently~\citep{heer2010tour}. By documenting these visual insights, practitioners can ensure that the knowledge encapsulated in the visualizations, along with its interpretation and the subsequent decisions made, is preserved and consistently applied throughout the ML lifecycle. \DIFdelbegin \DIFdel{Although writing }\DIFdelend \DIFaddbegin \DIFadd{Writing }\DIFaddend documentation from implicit \DIFdelbegin \DIFdel{and explicit feedback mechanisms still }\DIFdelend \DIFaddbegin \DIFadd{feedback mechanisms and keeping the documentation up-to-date }\DIFaddend requires significant manual effort\DIFdelbegin \DIFdel{and resources, }\DIFdelend \DIFaddbegin \DIFadd{. Therein also lies an opportunity for notebook designers and developers to incorporate automated techniques for generating such documentation. We hope that }\DIFaddend the dataset contributed by this study provides a valuable resource \DIFdelbegin \DIFdel{for ongoing efforts to automatically generate documentation from code written in Jupyter notebooks~\mbox{%DIFAUXCMD
\citep{yang2021subtle}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{to develop and evaluate such automated tools}\DIFaddend .

\subsection{Automated Data Validation}

Catching data errors \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend critical since ML models trained using dirty data will lead to poor and incorrect predictions. This is a cause for concern since these models may be deployed in production and their output may be used by other services which will then also perform poorly. The output of the ML models are often also used to capture new training data, thus leading to a feedback loop which over time will degrade the model performance~\citep{sculley2015hidden,breck2019data}.

\citet{breck2017ml} view ML training as analogous to compilation and recommend that training data be tested, similar to code in traditional software. They propose a rubric to assess the production readiness of an ML pipeline, which includes several recommendations for data testing. In subsequent work, \citet{breck2019data} introduce \emph{TensorFlow Data Validation}, an automated tool for data validation that works on principles borrowed from the database community. Data expectations are specified in a schema, and each new data batch is validated against this schema to ensure it meets the expected criteria or prompts schema updates if requirements change.

The results from RQ2 indicate that assertions can be used to automatically validate critical assumptions in ML workflows. Embedding assertions into the code helps ensure that any deviation from \DIFdelbegin \DIFdel{expected conditions }\DIFdelend \DIFaddbegin \DIFadd{the expected shape of the training data, presence of missing data, and absence of critical resources }\DIFaddend is promptly flagged. This reduces the reliance on manual verification thus enhancing the robustness and maintainability of ML pipelines.
\DIFaddbegin 

\DIFaddend Assertions also serve as an executable form of documentation that captures the assumptions and decisions made during model development. This is particularly important in collaborative environments where multiple practitioners might work on the same project. Assertions ensure that all team members have a clear understanding of the requirements and constraints across the entire ML pipeline, facilitating smoother transitions and handovers. \DIFdelbegin \DIFdel{Moreover, assertions may enhance the reproducibility of ML experiments by ensuring that the same validation checks are applied consistently, regardless of the execution order of the notebook cells~\mbox{%DIFAUXCMD
\citep{wang2020assessing}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{Future work can focus on providing empirical evidence to demonstrate that the integration of assertions significantly improves the reliability of ML workflows in practice}\DIFaddend .

Results from RQ2 and RQ3 show that assertions, print statements and last cell statements are predominantly used in the upstream, data-intensive stages of the MLDL. For instance, this study identifies several assertions that perform data validation checks to ensure data meets specific criteria or constraints. Similarly, we find assertions, print statements, and last cell statements that conduct type, shape, and distribution checks to safeguard against data inconsistencies. \DIFdelbegin \DIFdel{These resultsshow a missed opportunity in data validation tasks. We }\DIFdelend \DIFaddbegin \DIFadd{Based on these results, we }\DIFaddend argue that when integrating dedicated data validation tools into an operational ML pipeline, the existing feedback mechanisms in notebooks can be effectively used to write the data validation schema.

\subsection{ML Testing Literacy}

Despite the potential benefits, our results from RQ1 reveal that assertions are not widely used in Jupyter notebooks. The relatively low adoption rate compared to implicit feedback mechanisms, suggests that there is significant room for growth and improvement in the integration of assertions into ML development workflows.

We hypothesize that this limited use of assertions is due to a lack of knowledge among practitioners regarding testing approaches and the absence of adequate tooling to support the assertion of data and model properties. Guidelines and curricula on ML coding practices have historically focused on building better models. We argue that the focus must shift towards building reliable ML-enabled systems by incorporating decades of research from the software engineering community with ML. Our recommendation is evident in the recent developments within the software engineering research community that steer education and research towards addressing the specific challenges faced when developing ML-enabled software systems~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{kastner2020teaching}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{kastner2020teaching,lanubile2021towards,lanubile2023teaching}}\hskip0pt%DIFAUXCMD
}\DIFaddend . Fostering a deeper understanding of testing practices and providing tools tailored to ML development can help promote more rigorous and reliable testing methodologies within the community.

The assertions \DIFdelbegin \DIFdel{analysed }\DIFdelend \DIFaddbegin \DIFadd{analyzed }\DIFaddend in RQ1 and RQ2 further reveal two significant challenges in the current landscape of testing in ML Jupyter notebooks. First, the assertions identified in this study lack the support of dedicated testing libraries. This suggests that the culture of testing ML code is still in its infancy, and ML practitioners are not yet fully exploring or leveraging software testing libraries. Secondly, existing testing libraries are not specifically tuned to the needs of ML code, leading practitioners to rely on basic built-in approaches. This highlights the necessity for developing specialized testing frameworks that cater to the unique requirements of ML projects.

\subsection{Machine Learning Bugs}

The findings of this study align with prior research on real-world defects in ML systems. \citet{humbatova2020taxonomy} propose a taxonomy of faults in DL systems, while \citet{morovati2023bugs} curate \emph{Defects4ML}, a dataset consisting of ML bugs. Both studies identify defects arising from the use of incorrect data types and shapes, deprecated APIs, and missing GPU resources. Our study identifies several explicit assertions that can catch such data errors. Additionally, we find several implicit checks performed using print and last cell statements to verify the presence of compute resources and libraries, monitor model training, and check the architecture of neural networks. \DIFaddbegin \DIFadd{Future work should focus on mapping the feedback mechanisms in Jupyter notebooks to the ML defects identified by \mbox{%DIFAUXCMD
\citet{humbatova2020taxonomy} }\hskip0pt%DIFAUXCMD
and \mbox{%DIFAUXCMD
\citet{morovati2023bugs} }\hskip0pt%DIFAUXCMD
which can aid practitioners in debugging or preventing faults using the appropriate feedback technique.
}\DIFaddend 

This study further reveals several avenues for \DIFaddbegin \DIFadd{future }\DIFaddend research. We primarily focused on identifying feedback mechanisms used when writing ML code in Jupyter notebooks and subsequently exploring a limited subset of them. Further research is required to \DIFdelbegin \DIFdel{explore the }\DIFdelend \DIFaddbegin \DIFadd{investigate these }\DIFaddend feedback mechanisms in \DIFdelbegin \DIFdel{more detail and organize the knowledge }\DIFdelend \DIFaddbegin \DIFadd{greater depth and systematically organize them }\DIFaddend into a taxonomy \DIFdelbegin \DIFdel{based on }\DIFdelend \DIFaddbegin \DIFadd{grounded in }\DIFaddend specific ML validation tasks. \DIFdelbegin \DIFdel{Future work can focus on mapping the feedback mechanismsin Jupyter notebooks to the ML defects identified by \mbox{%DIFAUXCMD
\citet{humbatova2020taxonomy} }\hskip0pt%DIFAUXCMD
and \mbox{%DIFAUXCMD
\citet{morovati2023bugs} }\hskip0pt%DIFAUXCMD
which can aid practitioners in debugging or preventing faults using the appropriate feedback technique}\DIFdelend \DIFaddbegin \DIFadd{It is important to recognize that machine learning workflows vary significantly across different tasks---such as classification, regression, and clustering. Each task necessitates distinct feedback mechanisms. For instance, assertions regarding image dimensions are pertinent in computer vision, whereas data distribution checks are critical in natural language processing. By analyzing the dataset according to the specific types of ML tasks, researchers can obtain targeted insights, thereby enhancing the practicality and applicability of recommendations for practitioners}\DIFaddend .

\subsection{Code Quality and Conformance to Standards}

Results from RQ1 show that 68\% of the assertions do not contain a failure message. This can lead to problems such as unclear error diagnostics, difficulty in debugging, and increased time to identify and resolve issues. \citet{vidoni2021evaluating} investigated the testing culture in open-source R packages and identified several \DIFdelbegin \DIFdel{anti-patterns }\DIFdelend \DIFaddbegin \DIFadd{antipatterns }\DIFaddend that lead to the accumulation of testing technical debt. The dataset contributed by this study can be used by researchers to replicate the \citet{vidoni2021evaluating} study using assertions written in Python.

\DIFaddbegin \subsection{\DIFadd{Complexity of Notebooks}}

%DIF >  TODO: why is it important to analyze FMs wrt complexity of notebooks?
%DIF >  TBH I do not agree with the reviewer's comment about this...
\DIFadd{Analyzing the feedback mechanisms with respect to the complexity of Jupyter notebooks remains a promising direction for future research. We believe that the primary challenge here will be to develop an appropriate definition for notebook complexity.
}

\DIFadd{To the best of our knowledge, no formal definition or metrics to quantify notebook complexity exist to date. In their research, \mbox{%DIFAUXCMD
\citet{grotov2022large-scale} }\hskip0pt%DIFAUXCMD
investigated the complexity of Python code written inside Jupyter notebooks using a subset of 15 metrics derived from prior literature. These metrics however were primarily developed for code structured in an object-oriented programming paradigm. Consequently, the applicability of these metrics to code within notebooks is questionable, particularly given that research indicates notebooks frequently diverge from established software engineering best practices~\mbox{%DIFAUXCMD
\citep{pimentel2019large-scale}}\hskip0pt%DIFAUXCMD
.
}

\DIFadd{It is further unclear how metrics such as the size of the training data, complexity of the learning algorithm, computational costs and CO\(_2\) emissions associated with training can be incorporated into the definition for notebook complexity. Addressing these challenges would facilitate the stratification of our dataset into distinct categories---namely low, medium, and high complexity---which can enhance the depth of this study by enabling a more nuanced analysis of feedback mechanisms within this context.
}

\subsection{\DIFadd{Reproducibility of Jupyter Notebooks}}

\DIFadd{Code written within Jupyter notebooks can reach a convoluted state since the code cells can be executed in any order. Each time a cell is run, it updates the state of the program within the kernel, leading to the potential for discrepancies if cells defining variables or functions are executed after cells that depend on them. Such out-of-order execution may result in a situation where subsequent cells operate on outdated or undefined values, ultimately complicating the workflow and undermining the reproducibility of ML experiments~\mbox{%DIFAUXCMD
\citep{pimentel2019large-scale,wang2020assessing}}\hskip0pt%DIFAUXCMD
.
}

\DIFadd{The assertions presented in Section~\ref{sec:result-explicit} can be used to enhance the reproducibility of ML experiments by ensuring that consistent validation checks are applied, irrespective of the execution order of the notebook cells~\mbox{%DIFAUXCMD
\citep{wang2020assessing}}\hskip0pt%DIFAUXCMD
. Conversely, implicit feedback constructs like }{\color{blue}%DIFAUXCMD
\lstinline{print("Accuracy:", accuracy)} %
}%DIFAUXCMD
\DIFadd{are only effective if the cell defining }{\color{blue}%DIFAUXCMD
\lstinline{accuracy} %
}%DIFAUXCMD
\DIFadd{has been executed prior to the print statement. In this paper, we have partially examined how these dependencies influence the functionality of feedback mechanisms, highlighting the complexities involved in maintaining reproducibility in scenarios where cell execution order is not strictly linear. Further investigation in this area could provide important insights into optimizing the use of assertions and other feedback mechanisms in Jupyter notebooks, particularly in the context of diverse ML workflows.
}

\DIFaddend \section{Threats to Validity}\label{sec:threats}

\paragraph{\textbf{Internal Validity}} This study examines Jupyter notebooks written in Python, specifically focusing on machine learning code. We do this by only considering notebooks that utilize popular ML libraries. Consequently, we may miss notebooks that are ML-related but do not use these specific libraries. Additionally, notebooks that import popular ML libraries but do not actively use them in the code could lead to misclassification. To mitigate this threat, we specifically look for ML libraries that facilitate model training rather than more general-purpose libraries such as pandas, numpy, and scipy, which can be used for a variety of non-ML tasks.

We enforced a time resource limit of 200 hours for conducting the case studies in RQ2 and RQ3, potentially constraining the depth and breadth of our investigation. \DIFdelbegin \DIFdel{The }\DIFdelend \DIFaddbegin \DIFadd{Although alternative stopping rules such as theoretical saturation are common, they are inherently subjective and can vary based on the judgement of a researcher. In contrast, a time-bound method ensures transparency and reflects the practical resource constraints in real-world research. Additionally, the }\DIFaddend stratified random sampling used for selecting the case study candidates mitigates this threat by ensuring that a representative cross-section of the population is sampled within the time constraint. While this sampling method enhances the generalizability of our results, the time limitation should still be considered when interpreting and applying the study's findings.

The case studies conducted in RQ2 and RQ3 are subject to bias from the first author's individual perspective and understanding which could limit the interpretation of data and potentially overlook alternative viewpoints or explanations. To mitigate this threat, we implemented a rigorous review process involving multiple rounds of discussion with the second and third authors. These discussions continued until a consensus was reached on the interpretations and findings.

\paragraph{\textbf{External Validity}} Our analysis is based on ML notebooks \DIFaddbegin \DIFadd{that are }\DIFaddend publicly available on \DIFaddbegin \DIFadd{platforms such as }\DIFaddend GitHub and Kaggle. \DIFdelbegin \DIFdel{Consequently}\DIFdelend \DIFaddbegin \DIFadd{As a result}\DIFaddend , the findings may not be generalizable to Python notebooks that do not \DIFdelbegin \DIFdel{concentrate on ML . Additionally, the results may not be applicable }\DIFdelend \DIFaddbegin \DIFadd{center around ML applications. Furthermore, the applicability of the results is limited when it comes }\DIFaddend to notebooks authored in other statistical programming languages\DIFdelbegin \DIFdel{(}\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend such as R or Julia\DIFdelbegin \DIFdel{), or to proprietary notebooks hosted on different platforms}\DIFdelend .

\DIFaddbegin \DIFadd{The analysis in this study is conducted only using Jupyter notebooks. However, the underlying workflow of users across all notebook environments is identical---that is, users execute chunks of code using a REPL mechanism. As such, the findings of this study may be generalizable to other notebook technologies.
}

\DIFadd{Finally, we would like to reiterate that the case studies presented in RQ2 and RQ3 serve to illustrate key themes, and no definitive conclusions should be drawn from individual instances.
}\DIFaddend \section{Related Work}\label{sec:related}

\subsection{Assertions and Software Bugs}

\citet{kochhar2017revisiting} conducted a partial replication of a prior study on assertion usage, expanding the analysis to 185 Java projects from GitHub. \citet{vidoni2021evaluating} conducted a comprehensive study on the unit testing practices within 177 systematically selected open-source R packages hosted on GitHub. The research addressed the quality of testing, identified testing goals, and pinpointed potential sources of testing technical debt.

\citet{humbatova2020taxonomy} developed a comprehensive taxonomy of faults in deep learning systems through the manual analysis of 1059 \DIFdelbegin \DIFdel{artefacts }\DIFdelend \DIFaddbegin \DIFadd{artifacts }\DIFaddend from GitHub and Stack Overflow, complemented by structured interviews with 20 researchers and practitioners. The study categorized faults into five top-level categories, encompassing model, data, training, deployment, and infrastructure-related faults. \citet{morovati2023bugs} addressed the challenge of assessing the quality and performance of tools designed to improve the reliability of machine learning components by creating a standard fault benchmark called defect4ML. This benchmark consists of 100 reproducible bugs reported by ML developers on GitHub and Stack Overflow, focusing on the TensorFlow and Keras ML frameworks.

In contrast to prior work, our research focuses on the feedback mechanisms used within Jupyter notebooks for ML development. While previous work has provided insights into assertion usage and fault taxonomies, our study specifically examines the role of implicit and explicit feedback in ensuring data and model integrity within interactive development environment provided by Jupyter notebooks.

\subsection{Computational Notebooks and Software Engineering}\label{sec:notebooks}

\citet{psallidas2019data} provide an overview of the evolving landscape of computational notebooks by \DIFdelbegin \DIFdel{analysing }\DIFdelend \DIFaddbegin \DIFadd{analyzing }\DIFaddend six million Python notebooks, two million enterprise Data Science (DS) pipelines, source code and metadata from over 900 releases of 12 important DS libraries. Their findings can be used by system builders for improving DS tools and also by DS practitioners to understand the current trends in technologies they should focus on. \citet{pimentel2019large-scale} mined 1.4 million notebooks from GitHub to conduct an empirical study on the coding practices in computational notebooks. Based on their analysis, the authors propose guidelines on improving reproducibility of computational notebooks. To enable future research on computational notebooks, \citet{quaranta2021kgtorrent} mine Kaggle and present \textit{KGTorrent}, a public dataset consisting of approximately 250 thousand Jupyter notebooks written in Python. The dataset also contains a relational database dump of metadata regarding publicly available notebooks on Kaggle.

Studies with human subjects have been conducted to gain a deeper understanding of the challenges faced by ML practitioners when developing ML models inside notebooks. The results indicate that ML practitioners work in a highly iterative fashion, often experimenting with multiple strategies to \DIFdelbegin \DIFdel{analyse }\DIFdelend \DIFaddbegin \DIFadd{analyze }\DIFaddend the data or produce meaningful visualizations~\citep{kandel2012enterprise,kery2018story,liu2019understanding,chattopadhyay2020whats}. Studies have also been conducted to understand how practitioners generate, evaluate and manage alternative hypothesis, visual designs, methods, tools, algorithms and data sources to arrive at the final implementation~\citep{liu2019understanding,kandel2012enterprise}. The findings from these studies can be used as guidelines for improving existing notebook technologies or designing new ones.

To manage and prune multiple versions of code that accumulate over time when developing in notebooks, \citet{head2019managing} propose a code gathering tool that allows practitioners to review and only keep the relevant version of code. Other tools such as \textit{WrangleDoc} and \textit{VizSmith} have also been proposed to aid ML practitioners when working within computational notebooks~\citep{yang2021subtle,bavishi2021vizsmith}.

Jupyter notebooks have been widely adopted by the DS and ML communities to develop ML pipelines~\citep{wang2020assessing,pimentel2019large-scale,quaranta2021kgtorrent}. Computational notebooks provide a rich source of data in the form of natural text, code and visualizations. Computational notebooks also allow ML practitioners to present the knowledge gained from the analysis in a narrative that can be shared with others~\citep{rule2018exploration}.

Our study builds on this foundation by focusing on the specific feedback mechanisms used within Jupyter notebooks during ML development. We examine both implicit and explicit feedback methods, providing a detailed analysis of their usage and effectiveness in maintaining robust ML workflows.

\subsection{ML Testing}\label{sec:ml-testing}

Existing scientific literature on ML testing broadly focuses on two aspects. First, on functional properties such as correctness and robustness of the model towards unseen data. And second, on non-functional properties such as fairness, interpretability and privacy~\citep{zhang2022machine,mehrabi2021survey,chen2022fairness}.

To test the correctness of ML models, several improvements over existing test adequacy metrics have been proposed. Tools such as \textit{DeepXplore} and \textit{DeepGauge} propose new test adequacy metrics such as neuron coverage adapted for ML enabled software systems~\citep{pei2017deepxplore,ma2018deepgauge,gerasimou2020importance-driven}. Formal verification methods have also been proposed that try to provide formal guarantee of robustness against adversarial examples. Such methods however are only feasible for statistical ML models and become computationally expensive for more complex models such as deep neural networks~\citep{zhu2021deepmemory,baluta2021scalable}. Several studies have been conducted on generating and detecting adversarial inputs for ML models. Data augmentation techniques based on fuzzing, search based software testing and mutation testing have been proposed to generate adversarial examples that can be used during model training to improve its robustness~\citep{braiek2019deepevolution,gao2020fuzz,wang2021robot,zhang2020white-box}. It is however not possible to include all variations of adversarial examples into the training data. Thus, methods have been proposed to detect adversarial inputs during runtime~\citep{xiao2021self-checking,wang2020dissector,wang2019adversarial,berend2020cats}.

Despite these advancements, there is a notable gap in practical, day-to-day testing practices within ML development environments. \DIFdelbegin \DIFdel{Our study addresses this gap by analysing }\DIFdelend \DIFaddbegin \DIFadd{To the best of our knowledge, \mbox{%DIFAUXCMD
\citet{openja_empirical_2024} }\hskip0pt%DIFAUXCMD
is the only study that empirically investigates the test code utilized within 11 ML projects spanning four different application domains. Our study builds upon the findings of \mbox{%DIFAUXCMD
\citet{openja_empirical_2024} }\hskip0pt%DIFAUXCMD
by further analyzing }\DIFaddend the feedback mechanisms \DIFdelbegin \DIFdel{in Jupyter notebooks, highlighting }\DIFdelend \DIFaddbegin \DIFadd{within Jupyter notebooks. We highlight }\DIFaddend how assertions and print statements are \DIFdelbegin \DIFdel{used }\DIFdelend \DIFaddbegin \DIFadd{employed }\DIFaddend to validate data and model performance, and \DIFdelbegin \DIFdel{proposing ways to integrate }\DIFdelend \DIFaddbegin \DIFadd{propose strategies for integrating }\DIFaddend these practices more effectively into the ML development lifecycle.

\subsection{Visualizations and Machine Learning}\label{sec:visualizations}

As ML augment software systems in safety-critical domains, emphasis has been put into explainability of ML models. ML models and the underlying data is complex and multidimensional. To combat the ``curse of dimensionality'', visual analytics has been widely adopted by the ML community to understand the data and the internal workings of ML models~\citep{yuan2021survey,hohman2019visual,wexler2019what-if}.

Prior studies have been conducted to understand how visual analytics techniques are currently being used in ML. \citet{yuan2021survey} conduct a systematic review of 259 papers and propose a taxonomy of visual analytics techniques for ML. \citet{hohman2019visual} conduct a survey of visual analytics techniques for Deep Learning Models. The findings of the study indicate that visual analytics has been widely adopted in ML for model explanation, interpretation, debugging and improvement.

Several tools have been proposed by the visual analytics research community to aid practitioners in understanding how their ML models operate. \citet{wexler2019what-if} propose \textit{What-If}, a visual analytics tool to explore alternative hypotheses, generate counterfactual reasoning, investigate decision boundaries of the model and how change in data affects the model predictions. To reduce the cognitive load of ML practitioners during model building phase, \citet{amershi2019software} propose \textit{ModelTracker}. Given a trained model and a sample dataset, ModelTracker presents all the information in traditional summary statistics along with the performance of the model.

\textit{ESCAPE}, \textit{GAM Coach}, \textit{Angler} and \textit{Drava} are a few other tools that have been proposed to handle specialized use-cases such as identifying systematic errors in ML models, generating counterfactual explanations for Generalized Additive Models, prioritizing machine translation model improvements and relating human concepts with semantic dimensions extracted by ML models during disentangled representation learning~\citep{ahn2023escape,wang2023gam,robertson2023angler,wang2023drava}.

Unlike prior work that focuses on standalone visual analytics tools, our study explores how visualizations within Jupyter notebooks are used by ML practitioners. We \DIFdelbegin \DIFdel{analyse }\DIFdelend \DIFaddbegin \DIFadd{analyze }\DIFaddend the motivations behind creating these visualizations and the insights they provide, offering a nuanced understanding of their role in the iterative ML development process.

\section{Conclusion}

This study provides a comprehensive analysis of feedback mechanisms employed within Jupyter notebooks during the machine learning development lifecycle. Our findings indicate that while implicit feedback mechanisms such as print statements and last cell statements are extensively utilized, there is a significant gap in the adoption of explicit feedback mechanisms like assertions. This reliance on manual validation highlights the potential for automated data validation to enhance the robustness and maintainability of ML pipelines. This study also underscores the necessity for better documentation practices to mitigate technical debt and ensure the reproducibility of ML experiments. Feedback mechanisms are used throughout the ML development workflow to achieve more reliable and \DIFdelbegin \DIFdel{efficent }\DIFdelend \DIFaddbegin \DIFadd{efficient }\DIFaddend model development processes. Future work should focus on developing specialized tools and frameworks to support these practices, thereby advancing the field of ML engineering.

\section{Declarations}

\paragraph{\textbf{Conflict of Interest}} The authors declare that they have no conflict of interest.

\paragraph{\textbf{Data Availability Statement}} The dataset created and used in this study along with the replication package is shared publicly under the Creative Commons Attribution (CC-BY) \DIFdelbegin \DIFdel{licence}\DIFdelend \DIFaddbegin \DIFadd{license}\DIFaddend : https://doi.org/10.6084/m9.figshare.26372140.v1. The case study candidates \DIFdelbegin \DIFdel{analysed }\DIFdelend \DIFaddbegin \DIFadd{analyzed }\DIFaddend in RQ2 and RQ3 can be viewed in our online appendix: https://arumoy.me/shome2023notebook.

% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%% \bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{bibliography}   % name your BibTeX data base

\end{document}
% end of file template.tex
